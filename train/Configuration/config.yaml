#%% SEQ2SEQ CONFIG

zemberek_path: 'C:/Users/furka/Desktop/TurkishGrammarCorrection/data/zemberek/zemberek-full.jar'
zemberek_normalizer_path: 'C:/Users/furka/Desktop/TurkishGrammarCorrection/data/zemberek'
dataset:
    path: '../data/train/questions'
    train: 'train.csv'
    dev: 'dev.csv'
    test: 'test.csv'

    word_embeddings:
        use: True
        update: False


    max_length: 14
    max_vocab: 500000

model:

    pre_trained_model: 'C:/Users/furka/Desktop/TurkishGrammarCorrection/train/Experiments/ep3'

    char_embedding_size: 256
    bilstm_out_size: 256
    word_embedding_size: 300

    encoder_lstm_out_decoder_in: 256
    bidirectional: True

    device: 'cuda'
    # 'lstm' or 'gru'
    rnn_cell: 'lstm'

    # 'NLLL' for Negative log likelihood or 'Perp' for perplexity
    loss: 'NLLL'

    # 'Adam' or 'SGD'
    optimizer: 'Adam'

    scheduler:
        enabled: True
        rate: 1

    use_attention: True
    n_layers: 2
    dropout_input: 0.25
    dropout_output: 0.25

    variable_lengths: True

train:
    lr: 0.0013
    teacher_forcing_ratio: 1.25
    batch_size: 64
    epoch: 10
    early_stop_threshold: 2
    print_every: 500
    checkpoint_every: 1

save_dir: './Experiments'
