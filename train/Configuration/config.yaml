#%% SEQ2SEQ CONFIG


dataset:
    path: '../data/train/'
    train: 'train100k.csv'
    dev: 'dev.csv'
    test: 'test.csv'

    word_embeddings:
        use: True
        update: True


    max_length: 16
    max_vocab: 200000

model:
    char_embedding_size: 256
    bilstm_out_size: 256
    word_embedding_size: 300

    encoder_lstm_out_decoder_in: 256
    bidirectional: True

    device: 'cuda'
    # 'lstm' or 'gru'
    rnn_cell: 'lstm'

    # 'NLLL' for Negative log likelihood or 'Perp' for perplexity
    loss: 'NLLL'

    # 'Adam' or 'SGD'
    optimizer: 'Adam'

    scheduler:
        enabled: True
        rate: 1

    use_attention: True
    n_layers: 2
    dropout_input: 0.2
    dropout_output: 0.2

    variable_lengths: True

train:
    lr: 0.0012
    teacher_forcing_ratio: 1.25
    batch_size: 32
    epoch: 40
    early_stop_threshold: 2
    print_every: 250
    checkpoint_every: 4

save_dir: './Experiments'
